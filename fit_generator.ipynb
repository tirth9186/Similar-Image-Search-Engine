{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fit_generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzqVikwx8QE4",
        "outputId": "4ee1f5b1-16ea-4652-b367-4d7f574a1af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        }
      },
      "source": [
        "!pip install tensorflow==2.3\n",
        "!pip install keras==2.4.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.3 in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (0.35.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (0.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.1.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.18.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3) (1.12.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.7.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3) (0.4.8)\n",
            "Requirement already satisfied: keras==2.4.3 in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.4.3) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.4.3) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.4.3) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.4.3) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras==2.4.3) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orm0ZFDaEa54",
        "outputId": "706ddbe8-6e0b-491a-9302-59de2a7f1a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.35.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR5b3Ocm8QE8",
        "outputId": "dccc860d-6726-4973-e0eb-66ef24f961c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMzc0DdX8QE_",
        "outputId": "c3ac6062-8f78-4811-c702-f40a54692acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hmYjHnfzT2n"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import load\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm1kKgSVzYvt"
      },
      "source": [
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjES9BPwzAB9"
      },
      "source": [
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def find_max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fossAO5awWCU"
      },
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "    inputs1 = Input(shape=(1000,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "    decoder1 = add([fe2, se3])\n",
        "    drop_decoder1 = Dropout(0.5)(decoder1)\n",
        "    decoder2 = Dense(256, activation='relu')(drop_decoder1)\n",
        "    drop_decoder2 = Dropout(0.5)(decoder2)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(drop_decoder2)\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
        "    print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsilFOjaYbr7"
      },
      "source": [
        "def create_sequences(tokenizer,max_length,desc_list,photo,vocab_size):\n",
        "    X1,X2,y = list(),list(),list()\n",
        "    for desc in desc_list:\n",
        "        seq = tokenizer.texts_to_sequences([desc])\n",
        "        for i in range(1, len(seq[0])):\n",
        "            in_seq, out_seq = seq[0][:i], seq[0][i]\n",
        "            # print(in_seq,out_seq)\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)\n",
        "            X1.append(photo)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq.flatten())\n",
        "    return array(X1), array(X2), array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPhbRN7BnVQs"
      },
      "source": [
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
        "\t# loop for ever over images\n",
        "\twhile 1:\n",
        "\t\tfor key, desc_list in descriptions.items():\n",
        "\t\t\t# retrieve the photo feature\n",
        "\t\t\tphoto = photos[key][0]\n",
        "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
        "\t\t\tyield [in_img, in_seq],out_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FkNwocdzh9G",
        "outputId": "50867679-783b-4e67-c5c0-ed9927fa8dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# train dataset\n",
        "# load training dataset (6K)\n",
        "filename = '/content/drive/My Drive/College/IRS Innovative/flicker8k-dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "train_steps = len(train)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('/content/drive/My Drive/College/IRS Innovative/flicker8k-dataset/Flickr8k_text/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features('/content/drive/My Drive/College/IRS Innovative/flicker8k-dataset/Flickr8k_Dataset/2048_features_resnet.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQGr6glZzi5v",
        "outputId": "3db68960-c5ab-4801-bf79-96219369d81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# dev dataset\n",
        "\n",
        "# load test set\n",
        "filename = '/content/drive/My Drive/College/IRS Innovative/flicker8k-dataset/Flickr8k_text/Flickr_8k.devImages.txt'\n",
        "test = load_set(filename)\n",
        "test_steps = len(test)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions('/content/drive/My Drive/College/IRS Innovative/flicker8k-dataset/Flickr8k_text/descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features('/content/drive/My Drive/College/IRS Innovative/flicker8k-dataset/Flickr8k_Dataset/2048_features_resnet.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTyRaKkkZZy1",
        "outputId": "3d3c8c39-3f87-409d-f41f-c9f5290a47e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = find_max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoZpn2S79yky"
      },
      "source": [
        "train_generator = data_generator(train_descriptions,train_features,tokenizer,max_length,vocab_size)\n",
        "test_generator = data_generator(test_descriptions,test_features,tokenizer,max_length,vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ3UHbenzpPL",
        "outputId": "43c01e6d-6ee5-49c9-fbd2-8f49927fb1dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "filepath = '/content/drive/My Drive/College/IRS Innovative/2048_ResNet50/model_extra-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=filepath,monitor='val_loss', verbose=1, save_best_only=True, mode='min'),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,patience=3, min_lr=0.00001,mode='min')\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 34)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1024)         2098176     dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 34, 256)      525312      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 1024)         0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 34, 256)      0           lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 256)          262400      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 256)          525312      dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 256)          0           dense_5[0][0]                    \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 256)          0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          65792       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 256)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 7579)         1947803     dropout_11[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 7,365,019\n",
            "Trainable params: 7,365,019\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr1472cf8QFb"
      },
      "source": [
        "# fit model\n",
        "history=model.fit(train_generator, epochs=50,steps_per_epoch=train_steps, verbose=1, callbacks=my_callbacks, validation_data=test_generator,validation_steps=test_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEg7xQRvkPMo"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm4g9aK-buyI"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOrQ_SJUmoQw"
      },
      "source": [
        "plt.savefig('/content/drive/My Drive/College/IRS Innovative/2048_ResNet50/result_graph.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}